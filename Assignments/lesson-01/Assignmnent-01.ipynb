{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我的作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {无人驾驶、语音识别、推荐系统}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {Github用于我们的代码托管，同时我们的作业也要放在Github上供批改；使用Jupyter的原因是它适合做交互调试和演示，使用Pycharm的原因是它适合做大型项目}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:概率模型是我们对于多个选项，利用模型计算其概率，选取概率最大的作为最优选项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:人脸识别、无人驾驶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:我们使用概率是在很多问题中靠逻辑难以抉择，至于后面的问题我不太懂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 P(w_1,w_2,…,w_m) 。其实就是想办法找到一个概率分布，它可以表示任意一个句子或序列出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:自动翻译、语音识别、对话机器人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:就是只关心单个单词出现的频率，或者认为单个单词出现的频率与其旁边的单词无关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:缺点是不够准确，与实际情况不符；优点是简单直观"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:每个单词出现的概率至于和它相邻的单词有关而与其他单词无关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hero = \"\"\"\n",
    "hero = 自我介绍 台词 询问\n",
    "自我介绍 = 寒暄* 我是 名字 | 寒暄* 我是 外号\n",
    "寒暄* = null | 寒暄 寒暄*\n",
    "寒暄 = 你好， | 很高兴认识你， | 大唐欢迎你，\n",
    "名字 = 李白。 | 钟馗。 | 李元芳。\n",
    "外号 = 剑仙。 | 地府判官。 | 王都密探。\n",
    "台词 = 大河之剑天上来！ | 对付魑魅魍魉，乃是强迫症最佳疗法！ | 密探的小本本上羞答答，人生太复杂！\n",
    "询问 = 你是来跟我争天下第一的吗？ | 你是什么鬼？ | 你说的每一句话都将作为呈堂证供！\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_comments = \"\"\"\n",
    "movie_comments = 演员 动作 形容\n",
    "演员 = 大宝 | 书记 | 大宝 | 京哥\n",
    "动作 = 演的 | 表现 | 展现 | 演绎\n",
    "形容 = 好 | 不好 | 太棒了 | 太烂了 |很尴尬\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。\n",
    "\n",
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammer(grammer_str,linesplit = '\\n',split = '='):\n",
    "    grammer = {}\n",
    "    for line in grammer_str.split(linesplit):\n",
    "        if  not line.strip(): continue\n",
    "        exp,stmt = line.split(split)\n",
    "        grammer[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hero': [['自我介绍', '台词', '询问']],\n",
       " '自我介绍': [['寒暄*', '我是', '名字'], ['寒暄*', '我是', '外号']],\n",
       " '寒暄*': [['null'], ['寒暄', '寒暄*']],\n",
       " '寒暄': [['你好，'], ['很高兴认识你，'], ['大唐欢迎你，']],\n",
       " '名字': [['李白。'], ['钟馗。'], ['李元芳。']],\n",
       " '外号': [['剑仙。'], ['地府判官。'], ['王都密探。']],\n",
       " '台词': [['大河之剑天上来！'], ['对付魑魅魍魉，乃是强迫症最佳疗法！'], ['密探的小本本上羞答答，人生太复杂！']],\n",
       " '询问': [['你是来跟我争天下第一的吗？'], ['你是什么鬼？'], ['你说的每一句话都将作为呈堂证供！']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hero_grammer = create_grammer(hero)\n",
    "hero_grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "choice = random.choice\n",
    "def generate(gram, target):\n",
    "    if target not in gram: return target # means target is a terminal expression\n",
    "    \n",
    "    expaned = [generate(gram, t) for t in choice(gram[target])]\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'很高兴认识你，大唐欢迎你，我是钟馗。密探的小本本上羞答答，人生太复杂！你是什么鬼？'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(hero_grammer,'hero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(n,gram,target):\n",
    "    for i in range(n):\n",
    "        print(generate(gram,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，我是钟馗。大河之剑天上来！你是来跟我争天下第一的吗？\n",
      "大唐欢迎你，你好，你好，我是钟馗。大河之剑天上来！你说的每一句话都将作为呈堂证供！\n",
      "很高兴认识你，很高兴认识你，我是李元芳。大河之剑天上来！你是什么鬼？\n",
      "大唐欢迎你，大唐欢迎你，你好，我是钟馗。大河之剑天上来！你是来跟我争天下第一的吗？\n",
      "很高兴认识你，大唐欢迎你，我是李元芳。密探的小本本上羞答答，人生太复杂！你是来跟我争天下第一的吗？\n"
     ]
    }
   ],
   "source": [
    "generate_n(5,hero_grammer,'hero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大宝演的太棒了\n",
      "大宝表现不好\n",
      "京哥表现不好\n",
      "书记展现太棒了\n",
      "大宝表现很尴尬\n"
     ]
    }
   ],
   "source": [
    "generate_n(5,create_grammer(movie_comments),'movie_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的prob_2函数，我们更换一个文本数据源，获得新的Language Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业使用的是数据集2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:/Users/13081/Desktop/movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anoconda_Python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(filename,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = data['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):return re.findall('\\w+',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = [''.join(token(str(a))) for a in comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    # we will learn the regular expression next course.\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = [''.join(token(str(a))) for a in comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'开篇长镜头惊险大气引人入胜结合了水平不俗的快剪下实打实的真刀真枪让人不禁热血沸腾特别弹簧床架挡炸弹空手接碎玻璃弹匣割喉等帅得飞起就算前半段铺垫节奏散漫主角光环开太大等也不怕作为一个中国人两个小时弥漫着中国强大得不可侵犯的氛围还是让那颗民族自豪心砰砰砰跳个不停'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_clean[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = [cut_string(i) for i in comments_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\13081\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.555 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "Token = []\n",
    "for i in range(len(comments_clean)):\n",
    "    Token+= cut(comments_clean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['感觉', '挺', '搞笑', '的', '战狼', '2', '里', '吴京', '这么', '能']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Token[500:510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328262),\n",
       " ('了', 102420),\n",
       " ('是', 73106),\n",
       " ('我', 50338),\n",
       " ('都', 36255),\n",
       " ('很', 34712),\n",
       " ('看', 34022),\n",
       " ('电影', 33675),\n",
       " ('也', 32065),\n",
       " ('和', 31290)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count = Counter(Token)\n",
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = [str(t) for t in Token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['真让人']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN[-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490313"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490312"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心', '恶心想']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_2_GRAM[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    if word in TOKEN:\n",
    "        return words_count[word]/len(TOKEN)\n",
    "    else:\n",
    "        return 1/len(TOKEN)\n",
    "words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "def prob_2(word1,word2):\n",
    "    if word1+word2 in TOKEN_2_GRAM:\n",
    "        return words_count_2[word1+word2]/len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1/len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.137936072148216e-05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('我们', '在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_prob = 1\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_word = words[i+1]\n",
    "        probability_1 = prob_1(next_word)\n",
    "        probability_2 = prob_2(word, next_word)\n",
    "        \n",
    "        sentence_prob *= (probability_2 / probability_1)\n",
    "    sentence_prob *= probability_1\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.700447371998775e-11"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('今天是个好日子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 书记表现好 with Prb: 6.068166860439823e-10\n",
      "sentence: 大宝演的很尴尬 with Prb: 1.3695993405748147e-12\n",
      "sentence: 大宝表现不好 with Prb: 1.2136333720879645e-10\n",
      "sentence: 大宝表现太烂了 with Prb: 6.974905990610049e-11\n",
      "sentence: 大宝演绎好 with Prb: 8.02528734335645e-10\n",
      "sentence: 书记表现很尴尬 with Prb: 1.4359280275402658e-11\n",
      "sentence: 大宝演的很尴尬 with Prb: 1.3695993405748147e-12\n",
      "sentence: 大宝演的太棒了 with Prb: 7.38923573960985e-12\n",
      "sentence: 大宝表现不好 with Prb: 1.2136333720879645e-10\n",
      "sentence: 大宝演绎好 with Prb: 8.02528734335645e-10\n"
     ]
    }
   ],
   "source": [
    "for sen in [generate(create_grammer(movie_comments,split = '='),'movie_comments') for i in range(10)]:\n",
    "    print('sentence: {} with Prb: {}'.format(sen, get_probability(sen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成n个句子，并能选择一个最合理的句子:\n",
    "\n",
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(grammer,target,linesplit,split,model,n): \n",
    "    sentences = [generate(create_grammer(grammer,linesplit,split),target) for i in range(n)]\n",
    "    prob = [model(sentence) for sentence in sentences]\n",
    "    sens = enumerate(prob)\n",
    "    sens_sorted = sorted(sens,key=lambda x: x[1],reverse = True)\n",
    "    return sentences[sens_sorted[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是李元芳。对付魑魅魍魉，乃是强迫症最佳疗法！你是什么鬼？'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(hero,'hero','\\n','=',get_probability,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:这个模型有什么问题？ 你准备如何提升？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:问题：2-gram假设不一定完全合理，数据的数量还不太够，同时数据来自影评，也会对句子合理性的判断造成影响。提升：增加数据量，或者说增加其他环境下的语料"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
